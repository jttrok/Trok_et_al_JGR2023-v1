{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1824e4cf-0af2-43c7-9c8a-95ce040137bd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "proj_dir='/path/to/main_project_folder/' # edit this line\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import dask.array as da\n",
    "import glob as glob\n",
    "import sys\n",
    "sys.path.append(proj_dir) \n",
    "from project_utils import parameters as param\n",
    "from project_utils import load_region\n",
    "from project_utils import prepare_inputs\n",
    "import importlib\n",
    "importlib.reload(param)\n",
    "importlib.reload(load_region)\n",
    "importlib.reload(prepare_inputs)\n",
    "\n",
    "def prep_ncep_files(ds):\n",
    "        ds = ds.drop(['gw', 'lat_bnds', 'lon_bnds', 'area', 'time_bnds', 'level_bnds'], errors='ignore')\n",
    "        return ds\n",
    "\n",
    "def prep_era5_files(ds):\n",
    "        ds['lat'] = xr.open_dataset(z_era5_files[5]).lat\n",
    "        ds = ds.swap_dims({'latitude':'lat', 'longitude':'lon'}).drop(['gw', 'lat_bnds', 'lon_bnds', 'area'])\n",
    "        return ds\n",
    "\n",
    "region_list = ['northcentral_north_america', \n",
    "               'southcentral_north_america', \n",
    "               'southeastern_north_america', \n",
    "               'southwestern_europe', \n",
    "               'western_europe', \n",
    "               'central_europe', \n",
    "               'eastern_europe', \n",
    "               'northeastern_europe', \n",
    "               'northeastern_asia', \n",
    "               'southeastern_asia', \n",
    "               'northsouthern_south_america', \n",
    "               'southsouthern_south_america', \n",
    "               'southwestern_africa', \n",
    "               'southeastern_africa', \n",
    "               'southwestern_australia', \n",
    "               'southeastern_australia', \n",
    "               'west_texas',\n",
    "               'east_texas',\n",
    "              ]\n",
    "\n",
    "hgt_ncep_files = sorted(glob.glob(\"../input_data_NCEP/NCEP-R2/hgt/*.nc\"))\n",
    "soilw_ncep_files = sorted(glob.glob(\"../input_data_NCEP/NCEP-R2/soilw/*.nc\"))\n",
    "tmax_ncep_files = sorted(glob.glob(\"../input_data_NCEP/NCEP-R2/tmax/*.nc\"))\n",
    "    \n",
    "for dset in ['ERA5', 'NCEP']:\n",
    "    for jj in range(len(region_list)):\n",
    "        region_str = region_list[jj]\n",
    "        print(jj)\n",
    "        print(region_str)\n",
    "\n",
    "        hem, region_input_lat_bbox, region_input_lon_bbox, region_box_x, region_box_y, region_lat, region_lon, region_lon_EW, region_t62_lats, region_t62_lons = load_region.load_region_constants(region_str)        \n",
    "\n",
    "        era5_path = \"../input_data_ERA5/\"+region_str+\"/daily/\"\n",
    "        z_era5_files = sorted(glob.glob(era5_path+\"z/*t62.nc\"))\n",
    "        soilw_era5_files = sorted(glob.glob(era5_path+\"swvl1/*t62.nc\"))\n",
    "        tmax_era5_files = sorted(glob.glob(era5_path+\"tmax/*t62.nc\"))\n",
    "        \n",
    "        ######################### 500-hPa geopotential height: #############################\n",
    "        # read daily data, subtract domain average 500-hPa trend, calculate daily standardized anomalies\n",
    "        \n",
    "        if dset == 'ERA5':\n",
    "            hgt_files = z_era5_files\n",
    "            soilw_files = soilw_era5_files\n",
    "            tmax_files = tmax_era5_files\n",
    "            prep_files = prep_era5_files\n",
    "        elif dset == 'NCEP':\n",
    "            hgt_files = hgt_ncep_files\n",
    "            soilw_files = soilw_ncep_files\n",
    "            tmax_files = tmax_ncep_files\n",
    "            prep_files = prep_ncep_files\n",
    "        else:\n",
    "            hgt_files = None\n",
    "            prep_files = None\n",
    "            print(\"DATASET NOT FOUND!\")\n",
    "\n",
    "        if isinstance(region_input_lon_bbox, slice): \n",
    "            hgt_ds = xr.open_mfdataset(hgt_files, combine = 'nested', concat_dim='time', preprocess=prep_files).sel(\n",
    "                lat = region_input_lat_bbox, \n",
    "                lon = region_input_lon_bbox,\n",
    "                time = param.time_period)\n",
    "            if dset == 'NCEP':\n",
    "                hgt_ds = hgt_ds.sel(level = param.hgt_level)\n",
    "            \n",
    "        else:\n",
    "            hgt_ds_left = xr.open_mfdataset(hgt_files, combine = 'nested', concat_dim='time', preprocess=prep_files).sel(\n",
    "                lat = region_input_lat_bbox, \n",
    "                lon = region_input_lon_bbox[0], \n",
    "                time = param.time_period)\n",
    "            hgt_ds_right = xr.open_mfdataset(hgt_files, combine = 'nested', concat_dim='time', preprocess=prep_files).sel(\n",
    "                lat = region_input_lat_bbox, \n",
    "                lon = region_input_lon_bbox[1],\n",
    "                time = param.time_period)\n",
    "            if dset == 'NCEP':\n",
    "                hgt_ds_left = hgt_ds_left.sel(level = param.hgt_level)\n",
    "                hgt_ds_right = hgt_ds_right.sel(level = param.hgt_level)\n",
    "            \n",
    "            hgt_ds = xr.concat([hgt_ds_left, hgt_ds_right], dim=\"lon\")\n",
    "\n",
    "        if dset == 'ERA5':\n",
    "            hgt_ds = hgt_ds.copy() / 9.80665 # convert geopotential to GPH\n",
    "            hgt_ds = hgt_ds.rename({'z':'hgt'})\n",
    "            \n",
    "        print(hgt_ds)\n",
    "        lats = hgt_ds['lat']\n",
    "        lons = hgt_ds['lon']\n",
    "\n",
    "        print('removing hgt trend')\n",
    "        ## calculate daily area-weighted domain average 500-hPa GPH\n",
    "        area_weights = xr.broadcast(np.cos(np.deg2rad(lats)), hgt_ds, exclude = ['lat', 'time'])[0]\n",
    "        hgt_domain_mean = hgt_ds['hgt'].weighted(area_weights.lat).mean(dim = ['lat', 'lon'])\n",
    "        ## calculate annual domain average 500-hPa GPH to remove seasonal variability \n",
    "        hgt_domain_mean = hgt_domain_mean.groupby('time.year').mean(dim = \"time\").to_dataframe(name = \"hgt\")\n",
    "        ## calculate linear trend in 500-hPa GPH\n",
    "        hgt_trend = np.polyfit(hgt_domain_mean['hgt'].index.get_level_values('year'), hgt_domain_mean['hgt'], 1)\n",
    "        print(\"Slope of hgt_trend: \", hgt_trend[0], \"m per year\")\n",
    "        ## calculate detrended hgt\n",
    "        hgt_ds['change'] = (hgt_ds.time.dt.year - 1979)*hgt_trend[0]\n",
    "        hgt_ds['hgt_detrend'] = hgt_ds['hgt'] - hgt_ds['change']\n",
    "        hgt_ds = hgt_ds.drop_vars('change')\n",
    "        ## calculate calendar-day standardized anomalies \n",
    "        hgt_ds['mean_detrend'] = hgt_ds['hgt_detrend'].groupby('time.dayofyear').mean(dim = 'time')\n",
    "        hgt_ds['sd_detrend'] = hgt_ds['hgt_detrend'].groupby('time.dayofyear').std(dim = 'time')\n",
    "        hgt_ds['hgt_anom_no_trend'] = (hgt_ds['hgt_detrend'].groupby('time.dayofyear') - hgt_ds['mean_detrend']).groupby('time.dayofyear')/hgt_ds['sd_detrend']\n",
    "        ## save as netcdf files\n",
    "        hgt_ds['hgt_detrend'].to_netcdf(\"../processed_data_\"+dset+\"/\"+region_str+\"/hgt.nc\")\n",
    "        print(\"../processed_data_\"+dset+\"/\"+region_str+\"/hgt.nc\")\n",
    "        hgt_ds['hgt_anom_no_trend'].to_netcdf(\"../processed_data_\"+dset+\"/\"+region_str+\"/hgt_calday_anomalies.nc\")\n",
    "        print(\"../processed_data_\"+dset+\"/\"+region_str+\"/hgt_calday_anomalies.nc\")\n",
    "\n",
    "\n",
    "\n",
    "        ######################### Soil Moisture: #############################\n",
    "\n",
    "        # read daily data, calculate daily standardized anomalies\n",
    "        if isinstance(region_input_lon_bbox, slice):\n",
    "            soilw_ds = xr.open_mfdataset(soilw_files, combine = 'nested', concat_dim='time', preprocess=prep_files).sel(\n",
    "                lat = region_input_lat_bbox, \n",
    "                lon = region_input_lon_bbox,  \n",
    "                time = param.time_period).astype(np.float64)\n",
    "            \n",
    "            if dset == 'NCEP':\n",
    "                soilw_ds = soilw_ds.squeeze('level')\n",
    "            \n",
    "        else: # read east and west hemispheres separately\n",
    "            soilw_ds_left = xr.open_mfdataset(soilw_files, combine = 'nested', concat_dim='time', preprocess=prep_files).sel(\n",
    "                lat = region_input_lat_bbox, \n",
    "                lon = region_input_lon_bbox[0],  \n",
    "                time = param.time_period).astype(np.float64)\n",
    "            soilw_ds_right = xr.open_mfdataset(soilw_files, combine = 'nested', concat_dim='time', preprocess=prep_files).sel(\n",
    "                lat = region_input_lat_bbox, \n",
    "                lon = region_input_lon_bbox[1],  \n",
    "                time = param.time_period).astype(np.float64)\n",
    "            \n",
    "             if dset == 'NCEP':\n",
    "                soilw_ds_left = soilw_ds_left.squeeze('level')\n",
    "                soilw_ds_right = soilw_ds_right.squeeze('level')\n",
    "            \n",
    "            soilw_ds = xr.concat([soilw_ds_left, soilw_ds_right], dim=\"lon\")\n",
    "\n",
    "        if dset == 'ERA5':\n",
    "            soilw_ds = soilw_ds.rename({'swvl1':'soilw'})\n",
    "            \n",
    "        lats = soilw_ds['lat']\n",
    "        lons = soilw_ds['lon']\n",
    "\n",
    "        ## compute area-weighted mean over the entire input map\n",
    "        area_weights = xr.broadcast(np.cos(np.deg2rad(lats)), soilw_ds, exclude = ['lat', 'time'])[0]\n",
    "        soilw_domain_mean = soilw_ds['soilw'].weighted(area_weights.lat).mean(dim = ['lat', 'lon'])\n",
    "        soilw_domain_mean = soilw_domain_mean.groupby('time.year').mean(dim = \"time\").to_dataframe(name = \"soilw\")\n",
    "        ## calculate linear trend in soilw\n",
    "        soilw_trend = np.polyfit(soilw_domain_mean['soilw'].index.get_level_values('year'), soilw_domain_mean['soilw'], 1)\n",
    "        print(\"Slope of soilw_trend: \", soilw_trend[0], \"per year (units: Volumetric soil moisture content fraction )\")\n",
    "        print(\"40-year trend results in total change approx: \", np.round(40*soilw_trend[0] / soilw_ds['soilw'].std(dim = 'time').mean(dim = ['lat', 'lon']).values,3)*100  ,\" % of standard deviation\")\n",
    "        ## calculate detrended soilw\n",
    "        soilw_ds['change'] = (soilw_ds.time.dt.year - 1979)*soilw_trend[0]\n",
    "        soilw_ds['soilw_detrend'] = soilw_ds['soilw'] - soilw_ds['change']\n",
    "        soilw_ds = soilw_ds.drop_vars('change')\n",
    "        ## calculate calendar-day standardized anomalies         \n",
    "        soilw_ds['cal_day_mean'] = soilw_ds['soilw_detrend'].groupby('time.dayofyear').mean(dim = 'time')\n",
    "        soilw_ds['cal_day_sd'] = soilw_ds['soilw_detrend'].groupby('time.dayofyear').std(dim = 'time')\n",
    "        soilw_ds['soilw_daily_anom'] = (soilw_ds['soilw_detrend'].groupby('time.dayofyear') - soilw_ds['cal_day_mean']).groupby('time.dayofyear')/soilw_ds['cal_day_sd']\n",
    "        ## save as netcdf files\n",
    "        soilw_ds['soilw_daily_anom'].to_netcdf(\"../processed_data_\"+dset+\"/\"+region_str+\"/soilw_calday_anomalies.nc\")\n",
    "        print(\"../processed_data_\"+dset+\"/\"+region_str+\"/soilw_calday_anomalies.nc\")\n",
    "        soilw_ds['cal_day_sd'].to_netcdf(\"../processed_data_\"+dset+\"/\"+region_str+\"/soilw_calday_stdev.nc\")\n",
    "        print(\"../processed_data_\"+dset+\"/\"+region_str+\"/soilw_calday_stdev.nc\")\n",
    "        soilw_ds['cal_day_mean'].to_netcdf(\"../processed_data_\"+dset+\"/\"+region_str+\"/soilw_calday_mean.nc\")\n",
    "        print(\"../processed_data_\"+dset+\"/\"+region_str+\"/soilw_calday_mean.nc\")\n",
    "        soilw_ds['soilw_detrend'].to_netcdf(\"../processed_data_\"+dset+\"/\"+region_str+\"/soilw.nc\")\n",
    "        print(\"../processed_data_\"+dset+\"/\"+region_str+\"/soilw.nc\")\n",
    "\n",
    "\n",
    "        ######################### TMAX: #############################\n",
    "\n",
    "        # read daily data, calculate daily standardized anomalies\n",
    "        if isinstance(region_input_lon_bbox, slice):\n",
    "            tmax_ds = xr.open_mfdataset(tmax_files, combine = 'nested', concat_dim='time', preprocess=prep_files).sel(\n",
    "                                        lat = region_input_lat_bbox, \n",
    "                                        lon = region_input_lon_bbox, \n",
    "                                        time = param.time_period).drop(['level'], errors='ignore').astype(np.float64)\n",
    "        else: # read east and west hemispheres separately\n",
    "            tmax_ds_left = xr.open_mfdataset(tmax_files, combine = 'nested', concat_dim='time', preprocess=prep_files).sel(\n",
    "                                                lat = region_input_lat_bbox, \n",
    "                                                lon = region_input_lon_bbox[0], \n",
    "                                                time = param.time_period).drop(['level'], errors='ignore').astype(np.float64)\n",
    "            tmax_ds_right = xr.open_mfdataset(tmax_files, combine = 'nested', concat_dim='time', preprocess=prep_files).sel(\n",
    "                                                lat = region_input_lat_bbox, \n",
    "                                                lon = region_input_lon_bbox[1], \n",
    "                                                time = param.time_period).drop(['level'], errors='ignore').astype(np.float64)\n",
    "            tmax_ds = xr.concat([tmax_ds_left, tmax_ds_right], dim=\"lon\")\n",
    "\n",
    "        lats = tmax_ds['lat']\n",
    "        lons = tmax_ds['lon']\n",
    "        ## compute area-weighted mean over the entire input map\n",
    "        area_weights = xr.broadcast(np.cos(np.deg2rad(lats)), tmax_ds, exclude = ['lat', 'time'])[0]\n",
    "        tmax_domain_mean = tmax_ds['tmax'].weighted(area_weights.lat).mean(dim = ['lat', 'lon'])\n",
    "        tmax_domain_mean = tmax_domain_mean.groupby('time.year').mean(dim = \"time\").to_dataframe(name = \"tmax\")\n",
    "        ## calculate linear trend in tmax\n",
    "        tmax_trend = np.polyfit(tmax_domain_mean['tmax'].index.get_level_values('year'), tmax_domain_mean['tmax'], 1)\n",
    "        print(\"Slope of tmax_trend: \", tmax_trend[0], \"per year (units: Kelvin )\")\n",
    "        ## calculate detrended tmax\n",
    "        tmax_ds['change'] = (tmax_ds.time.dt.year - 1979)*tmax_trend[0]\n",
    "        tmax_ds['tmax_detrend'] = tmax_ds['tmax'] - tmax_ds['change']\n",
    "        tmax_ds = tmax_ds.drop_vars('change')\n",
    "        ## save as netcdf files    \n",
    "        tmax_ds['tmax_detrend'].to_netcdf(\"../processed_data_\"+dset+\"/\"+region_str+\"/tmax.nc\")\n",
    "        print(\"../processed_data_\"+dset+\"/\"+region_str+\"/tmax.nc\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f479d3-8631-434a-9a15-907c25666717",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
