{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ce5898-35d9-40f7-b912-20111af8da1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_dir='/path/to/main_project_folder/' # edit this line\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import xarray as xr\n",
    "import time\n",
    "import h5py\n",
    "import pandas as pd\n",
    "from denseweight import DenseWeight\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.random import set_seed as tf_set_seed\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers.schedules import InverseTimeDecay\n",
    "#from tensorflow.keras import optimizers\n",
    "from tensorflow.compat.v1.keras import optimizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow.compat.v1 as tf_compat_v1\n",
    "from scipy.stats import loguniform\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from matplotlib import colors\n",
    "import sys\n",
    "sys.path.append(proj_dir)\n",
    "from project_utils import parameters as param\n",
    "from project_utils import load_region\n",
    "from project_utils import prepare_inputs\n",
    "from project_utils import utils as util\n",
    "from project_utils import model_utils as mu\n",
    "import analysis_fxns as fxns\n",
    "import importlib\n",
    "importlib.reload(fxns)\n",
    "importlib.reload(param)\n",
    "importlib.reload(prepare_inputs)\n",
    "importlib.reload(util)\n",
    "importlib.reload(mu)\n",
    "importlib.reload(load_region)\n",
    "import multiprocessing as mp\n",
    "\n",
    "## set random seeds ##\n",
    "np.random.seed(101)\n",
    "random.seed(201)\n",
    "tf_set_seed(333)\n",
    "session_conf = tf_compat_v1.ConfigProto(device_count={'CPU': 24})\n",
    "sess = tf_compat_v1.Session(config=session_conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5397ab21-41e2-4361-a48b-266fe03fc56c",
   "metadata": {},
   "source": [
    "# Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f27c3e-b912-493d-9a57-a89191130555",
   "metadata": {},
   "outputs": [],
   "source": [
    "ninputs = 1 # number of geospatial CNN input channels (i.e., GPH and SM)\n",
    "w = 200 # window size for smoothing PDP curve\n",
    "SNOWFREE = True # remove winter months from training data\n",
    "ndense = 1\n",
    "ncnn = 1\n",
    "ncrp = 2\n",
    "conv_filters = 8\n",
    "dense_neurons = 32\n",
    "verb = 0\n",
    "curr_patience = 100\n",
    "curr_loss = 'mean_squared_error'\n",
    "\n",
    "region_list = ['northcentral_north_america', \n",
    "               'southcentral_north_america', \n",
    "               'southeastern_north_america', \n",
    "               'southwestern_europe', \n",
    "               'western_europe', \n",
    "               'central_europe', \n",
    "               'eastern_europe', \n",
    "               'northeastern_europe', \n",
    "               'northeastern_asia', \n",
    "               'southeastern_asia', \n",
    "               'northsouthern_south_america', \n",
    "               'southsouthern_south_america', \n",
    "               'southwestern_africa', \n",
    "               'southeastern_africa', \n",
    "               'southwestern_australia', \n",
    "               'southeastern_australia'\n",
    "              ]\n",
    "nlag_tpl = [1] # number of days to lag the soil moisture input behind the prediction day"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8aca410-d1cc-45d6-be5a-a2705d9f6041",
   "metadata": {},
   "source": [
    "# Train Convolutional Neural Networks and Calculate Partial Dependence Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a449271-e0db-4819-8b22-8a96782efe72",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for dset in [\"ERA5\", \"NCEP\"]:\n",
    "    for nlag in nlag_tpl:   \n",
    "        for ij, region_str in enumerate(region_list):\n",
    "            hem, region_input_lat_bbox, region_input_lon_bbox, region_box_x, region_box_y, region_lat, region_lon, region_lon_EW, region_t62_lats, region_t62_lons = load_region.load_region_constants(region_str)        \n",
    "            print('region: ',region_str)\n",
    "            \n",
    "            ####### load CNN hyperparameters ########  \n",
    "            if dset == \"ERA5\":\n",
    "                tts, activation_list, optimizer_list, lr_list, dw_list, decay_list, decay_steps, decay_rate, curr_batch, nepochs, reg_list = load_region.load_ERA5_region_cnn_hyperparams(region_str, shuffle_sm=False, nlag=nlag)\n",
    "            elif dset == \"NCEP\":\n",
    "                tts, activation_list, optimizer_list, lr_list, dw_list, decay_list, decay_steps, decay_rate, curr_batch, nepochs, reg_list = load_region.load_NCEP_region_cnn_hyperparams(region_str, shuffle_sm=False, nlag=nlag)\n",
    "            \n",
    "            x_dat_daily, y_dat, ind, caldays, time_vec = prepare_inputs.get_model_inputs(region_str, nlag, SNOWFREE, hemisphere=hem, dset=dset)\n",
    "            time_vec = pd.to_datetime(time_vec).reset_index(drop=True)\n",
    "\n",
    "            ######## Training / Testing Split ########    \n",
    "            rs = tts[0]\n",
    "            yrs = np.arange(1979, 2022) \n",
    "            yrs_train, yrs_eval = train_test_split(yrs, test_size=0.37, random_state=rs, shuffle=True) # splits yrs into 70% train, 30% test\n",
    "            yrs_test, yrs_unseen = train_test_split(yrs_eval, test_size=0.50, random_state=rs+1, shuffle=True) # splits eval yrs into 2/3 test 1/3 unseen\n",
    "            yrs_train = sorted(yrs_train)\n",
    "            yrs_test = sorted(yrs_test)\n",
    "            yrs_unseen = sorted(yrs_unseen)\n",
    "\n",
    "            dw_alpha = 1.0\n",
    "            dw = DenseWeight(alpha=dw_alpha)\n",
    "            sample_weights = dw.fit(y_dat.values)\n",
    "\n",
    "            x_train, x_test, x_unseen, \\\n",
    "                y_train, y_test, y_unseen, \\\n",
    "                ind_train, ind_test, ind_unseen, \\\n",
    "                sweight_train, sweight_test, sweight_unseen, \\\n",
    "                cday_train, cday_test, cday_unseen, \\\n",
    "                time_train, time_test, time_unseen = util.train_test_unseen_split_by_years(yrs_train, yrs_test, yrs_unseen,\n",
    "                                                                           x_dat_daily, y_dat,\n",
    "                                                                           ind, sample_weights, caldays, time_vec)\n",
    "\n",
    "            print(np.shape(x_train), np.shape(x_test), np.shape(x_unseen), \n",
    "                  np.shape(y_train), np.shape(y_test), np.shape(y_unseen), \n",
    "                  np.shape(ind_train), np.shape(ind_test), np.shape(ind_unseen), \n",
    "                  np.shape(sweight_train), np.shape(sweight_test), np.shape(sweight_unseen), \n",
    "                  np.shape(cday_train), np.shape(cday_test), np.shape(cday_unseen))\n",
    "\n",
    "            fig, ax = plt.subplots(1,1,figsize=(5,5))\n",
    "            nbins = 50\n",
    "            alph = 0.5\n",
    "            ax.hist(y_train, label=\"train\", bins=nbins, density=True, alpha=1)\n",
    "            ax.hist(y_test, label=\"test\", bins=nbins, density=True, alpha=alph)\n",
    "            ax.hist(y_unseen, label=\"unseen\", bins=nbins, density=True, alpha=alph/2)\n",
    "            ax.set_ylabel('count')\n",
    "            ax.set_xlabel('tmax (K)')\n",
    "            ax.legend()\n",
    "            ax.set_title(region_str+' temp distribution '+str(rs))\n",
    "            plt.show()\n",
    "\n",
    "            \n",
    "            ######## Remove SM Layer ########    \n",
    "            print(\"shapes with SM:\", np.shape(x_train), np.shape(x_test), np.shape(x_unseen))\n",
    "            print('removing SM input layer!')\n",
    "            x_train = x_train[:,:,:,0]\n",
    "            x_test = x_test[:,:,:,0]\n",
    "            x_unseen = x_unseen[:,:,:,0]\n",
    "            x_dat_daily = x_dat_daily[:,:,:,0]\n",
    "            print(\"shapes without SM:\", np.shape(x_train), np.shape(x_test), np.shape(x_unseen))\n",
    "            \n",
    "\n",
    "            ######## Set Training Constants ########    \n",
    "            callback = EarlyStopping(monitor='val_loss', patience=curr_patience, restore_best_weights = True)\n",
    "            curr_dw_alpha = dw_list[0]\n",
    "            dw = DenseWeight(alpha=curr_dw_alpha)\n",
    "            sample_weights = dw.fit(y_dat.values)\n",
    "            sweight_train = sample_weights[ind_train]\n",
    "            sweight_test = sample_weights[ind_test]\n",
    "            curr_reg = reg_list[0]\n",
    "            curr_act_func = activation_list[0]\n",
    "            curr_lr = lr_list[0]\n",
    "            dec_steps = decay_steps[0]\n",
    "            lr_sched = None\n",
    "            lr_sched = InverseTimeDecay(curr_lr, dec_step, decay_rate, staircase=True)\n",
    "            optimizer_dict = {'RMSprop':optimizers.RMSprop(learning_rate=lr_sched)}\n",
    "            op = optimizer_list[0]\n",
    "            curr_opt = optimizer_dict[op] \n",
    "            rand_seed = 0\n",
    "            \n",
    "            print('loss:',curr_loss)\n",
    "            print('denseweight:',curr_dw_alpha)\n",
    "            print('l2 regularization:',curr_reg)\n",
    "            print('activation function:',curr_act_func)\n",
    "            print('learning rate:',curr_lr)\n",
    "            print('lr decay steps:',dec_step)\n",
    "            print('optimizer:',op,curr_opt)\n",
    "            print('rand_seed:',rand_seed)\n",
    "            \n",
    "            # ensure training is reproducible\n",
    "            np.random.seed(101+pp)\n",
    "            random.seed(201+pp)\n",
    "            tf_set_seed(333+pp)\n",
    "            \n",
    "            # build CNN\n",
    "            model_daily = None\n",
    "            model_daily = mu.build_model(lr = lr_sched, conv_filters=conv_filters, \n",
    "                                        dense_neurons=dense_neurons, dense_layers = ndense, \n",
    "                                        cnn_layers = ncnn, conv_relu_pool_layers = ncrp, \n",
    "                                        activity_reg = curr_reg, input_channels = ninputs, \n",
    "                                        loss_str=curr_loss, opt=curr_opt, \n",
    "                                        act_func=curr_act_func, \n",
    "                                        nlats=len(x_dat_daily[0,:,0,0]), nlons=len(x_dat_daily[0,0,:,0]))\n",
    "\n",
    "            # train CNN\n",
    "            history_daily = model_daily.fit({\"stacked_input\" : x_train, \"calday\": cday_train}, y_train, \n",
    "                                        batch_size = curr_batch,\n",
    "                                        epochs = nepochs,\n",
    "                                        sample_weight = sweight_train, \n",
    "                                        validation_data = ({\"stacked_input\" : x_test, \"calday\": cday_test}, y_test, sweight_test), \n",
    "                                        verbose = 0,\n",
    "                                        callbacks = [callback])\n",
    "\n",
    "            tmax_predictions_daily = model_daily.predict({\"stacked_input\" : x_dat_daily, \"calday\": caldays})[:,0]\n",
    "            tmax_predictions_unseen = model_daily.predict({\"stacked_input\" : x_unseen, \"calday\": cday_unseen})[:,0]\n",
    "            tmax_predictions_train = model_daily.predict({\"stacked_input\" : x_train, \"calday\": cday_train})[:,0]\n",
    "            tmax_predictions_test = model_daily.predict({\"stacked_input\" : x_test, \"calday\": cday_test})[:,0]\n",
    "            \n",
    "            # Plot model skill #\n",
    "            fxns.model_skill(region_str=region_str,\n",
    "                            nlag=nlag,\n",
    "                            tmax_predictions_unseen=tmax_predictions_unseen, \n",
    "                            tmax_predictions_train=tmax_predictions_train, \n",
    "                            tmax_predictions_test=tmax_predictions_test, \n",
    "                            y_unseen=y_unseen,\n",
    "                            y_train=y_train,\n",
    "                            y_test=y_test,\n",
    "                            sweight_unseen=sweight_unseen,\n",
    "                            sweight_train=sweight_train,\n",
    "                            sweight_test=sweight_test,\n",
    "                            dset=dset)  \n",
    "            # save model weights #\n",
    "            fxns.save_model(region_str=region_str,\n",
    "                           nlag=nlag,\n",
    "                           model=model_daily,\n",
    "                           history=history_daily,\n",
    "                           tmax_predictions=tmax_predictions_daily,\n",
    "                           ind_test=ind_test,\n",
    "                           ind_unseen=ind_unseen,\n",
    "                           time_vec=time_vec,\n",
    "                           y_dat=y_dat, \n",
    "                           shuffle_sm=False,\n",
    "                           no_sm=True,\n",
    "                           dset=dset) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
